---
output: github_document
always_allow_html: true
---

<!-- README.md is generated from README.Rmd. Please edit that file -->

```{r, include = FALSE}
knitr::opts_chunk$set(
  collapse = TRUE,
  comment = "#>",
  echo = FALSE
)

library(table.glue)
library(tidyverse)
library(gt)

```

# rfvs-regression

<!-- badges: start -->
<!-- badges: end -->

The goal of rfvs-regression is to compare random forest variable selection techniques for continuous outcomes

# Datasets included

```{r}
read_csv("data/datasets_inclusion_chart.csv", 
         show_col_types = FALSE) %>% 
 gt(rowname_col = 'exclusion') %>% 
 tab_footnote(locations = cells_stub(rows = 8), 
              footnote = 'Many datasets are different versions of the same data')
```

# Central illustration

The experiment we ran involves three steps:

1. Select variables with a given method.
1. Fit a random forest to the data, including only selected variables.
1. Evaluate prediction accuracy of the forest in held-out data.

*Note*: We use both axis-based and oblique random forests for regression in step 2. 

We assume that better variable selection leads to better prediction accuracy. Results from the experiment are below.

```{r}

targets::tar_load(results_smry)


data_tbl <- results_smry %>% 
 ungroup() %>% 
 mutate(
  rfvs = str_remove(rfvs, 'rfvs_'),
  across(starts_with('n_selected'), ~ as.integer(round(.x))),
  across(starts_with('time'),  ~ as.numeric(.x, units = 'secs'))
 ) %>% 
 arrange(rsq_axis_50)

tbl_vars <- c('n_selected',
              'rsq_oblique', 
              'rsq_axis',
              'time')

for(t in tbl_vars){
 
 t25 <- data_tbl[[paste(t, "25", sep = '_')]]
 t50 <- data_tbl[[paste(t, "50", sep = '_')]]
 t75 <- data_tbl[[paste(t, "75", sep = '_')]]
 
 data_tbl[[t]] <- table_glue("{t25} ({t25} - {t75})")
 
}

data_tbl %>% 
 arrange(n_selected_50) %>% 
 select(rfvs, all_of(tbl_vars)) %>% 
 gt(rowname_col = 'rfvs') %>% 
 tab_stubhead("Variable selection method") %>% 
 cols_label(n_selected = 'N variables selected',
            rsq_oblique = 'Oblique',
            rsq_axis = 'Axis-based',
            time = 'Selection time, seconds') %>% 
 tab_spanner(columns = c("rsq_oblique", "rsq_axis"), 
             label = "R-squared (25th %, 75th %)") %>% 
 cols_align('center') %>% 
 cols_align('left', columns = 'rfvs')

```

*Takeaways*: 

1. Using oblique forests to select variables (the `aorsf` method for variable selection) leads to a parsimonious set of predictors with high prediction accuracy. Moreover, prediction accuracy is high whether the final model is oblique or axis-based.  

1. If the final model is axis-based rather than oblique, the axis-based variable selection methods do much better in terms of prediction accuracy.
